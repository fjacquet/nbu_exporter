---
phase: 01-critical-fixes-stability
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/exporter/client.go
  - internal/exporter/client_test.go
  - internal/exporter/interface.go
autonomous: true

must_haves:
  truths:
    - "NbuClient.Close() waits for active requests before closing connections"
    - "Exporter can be stopped and restarted multiple times without connection leaks"
    - "Close() uses context deadline to bound waiting time"
  artifacts:
    - path: "internal/exporter/client.go"
      provides: "Proper resource cleanup with drain logic"
      contains: "func (c *NbuClient) Close"
    - path: "internal/exporter/client_test.go"
      provides: "Tests for cleanup behavior"
      contains: "TestNbuClient.*Close"
  key_links:
    - from: "internal/exporter/client.go"
      to: "http.Transport"
      via: "CloseIdleConnections with proper drain"
      pattern: "CloseIdleConnections"
---

<objective>
Implement proper resource cleanup in NbuClient.Close() with connection draining

Purpose: Fix TD-05 (resource cleanup). Currently Close() just calls CloseIdleConnections() but doesn't wait for active requests to complete or drain pending connections. This can cause connection leaks when the exporter is restarted. Proper cleanup ensures graceful shutdown.

Output: NbuClient.Close() that properly drains active connections with a timeout
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@internal/exporter/client.go
@internal/exporter/client_test.go
@internal/exporter/interface.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement proper Close() with connection tracking</name>
  <files>internal/exporter/client.go</files>
  <action>
Implement proper resource cleanup in NbuClient:

1. Add connection tracking to NbuClient struct:
   ```go
   type NbuClient struct {
       client    *resty.Client
       cfg       models.Config
       tracer    trace.Tracer

       // Connection tracking for graceful shutdown
       mu            sync.Mutex
       activeReqs    int32         // Count of active requests
       closed        bool          // Whether Close() has been called
       closeChan     chan struct{} // Signaled when all requests complete
   }
   ```

2. Add request tracking in FetchData():
   ```go
   func (c *NbuClient) FetchData(ctx context.Context, url string, target interface{}) error {
       // Check if client is closed
       c.mu.Lock()
       if c.closed {
           c.mu.Unlock()
           return fmt.Errorf("client is closed")
       }
       atomic.AddInt32(&c.activeReqs, 1)
       c.mu.Unlock()

       defer func() {
           if atomic.AddInt32(&c.activeReqs, -1) == 0 {
               c.mu.Lock()
               if c.closed && c.closeChan != nil {
                   close(c.closeChan)
                   c.closeChan = nil
               }
               c.mu.Unlock()
           }
       }()

       // ... rest of existing FetchData implementation
   }
   ```

3. Update Close() to wait for active requests with timeout:
   ```go
   // Close releases resources associated with the HTTP client.
   // It waits for active requests to complete (up to the provided timeout)
   // before closing connections.
   //
   // Parameters:
   //   - ctx: Context for shutdown timeout. If nil, uses 30-second default timeout.
   //
   // Returns an error if:
   //   - The client is already closed
   //   - Timeout exceeded while waiting for active requests
   func (c *NbuClient) Close() error {
       c.mu.Lock()
       if c.closed {
           c.mu.Unlock()
           return fmt.Errorf("client already closed")
       }
       c.closed = true

       // Check if there are active requests
       activeCount := atomic.LoadInt32(&c.activeReqs)
       if activeCount > 0 {
           c.closeChan = make(chan struct{})
           c.mu.Unlock()

           // Wait for active requests with timeout
           ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
           defer cancel()

           select {
           case <-c.closeChan:
               log.Debug("All active requests completed during shutdown")
           case <-ctx.Done():
               log.Warnf("Timeout waiting for %d active requests during shutdown", activeCount)
           }
       } else {
           c.mu.Unlock()
       }

       // Close idle connections
       if c.client != nil {
           c.client.GetClient().CloseIdleConnections()
           c.client = nil
       }

       return nil
   }
   ```

4. Update NewNbuClient() to initialize new fields:
   ```go
   func NewNbuClient(cfg models.Config) *NbuClient {
       // ... existing code ...

       return &NbuClient{
           client:     client,
           cfg:        cfg,
           tracer:     tracer,
           activeReqs: 0,
           closed:     false,
       }
   }
   ```

5. Add CloseWithContext() for explicit timeout control:
   ```go
   // CloseWithContext releases resources with explicit timeout control.
   // Use this when you need custom shutdown timeout behavior.
   func (c *NbuClient) CloseWithContext(ctx context.Context) error {
       c.mu.Lock()
       if c.closed {
           c.mu.Unlock()
           return fmt.Errorf("client already closed")
       }
       c.closed = true

       activeCount := atomic.LoadInt32(&c.activeReqs)
       if activeCount > 0 {
           c.closeChan = make(chan struct{})
           c.mu.Unlock()

           select {
           case <-c.closeChan:
               log.Debug("All active requests completed during shutdown")
           case <-ctx.Done():
               log.Warnf("Context cancelled while waiting for %d active requests", activeCount)
               return ctx.Err()
           }
       } else {
           c.mu.Unlock()
       }

       if c.client != nil {
           c.client.GetClient().CloseIdleConnections()
           c.client = nil
       }

       return nil
   }
   ```
  </action>
  <verify>
Run `go build ./...` to ensure compilation succeeds.
Run `go test ./internal/exporter/... -v` to verify existing tests pass.
  </verify>
  <done>
NbuClient has connection tracking with activeReqs counter. Close() waits for active requests with 30-second timeout. CloseWithContext() allows custom timeout. FetchData tracks request lifecycle.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update interface and add cleanup tests</name>
  <files>internal/exporter/interface.go, internal/exporter/client_test.go</files>
  <action>
Update the interface and add comprehensive tests:

1. Optionally add CloseWithContext to interface (interface.go) if needed:
   ```go
   type NetBackupClient interface {
       FetchData(ctx context.Context, url string, target interface{}) error
       DetectAPIVersion(ctx context.Context) (string, error)
       Close() error
       // CloseWithContext provides shutdown with explicit timeout control
       // CloseWithContext(ctx context.Context) error  // Optional addition
   }
   ```

2. Add tests for Close() behavior (client_test.go):
   ```go
   func TestNbuClientCloseIdempotent(t *testing.T) {
       cfg := createBasicTestConfig("13.0", "test-key")
       client := NewNbuClient(cfg)

       // First close should succeed
       err := client.Close()
       if err != nil {
           t.Errorf("First Close() unexpected error: %v", err)
       }

       // Second close should return error
       err = client.Close()
       if err == nil {
           t.Error("Second Close() expected error, got nil")
       }
       if !strings.Contains(err.Error(), "already closed") {
           t.Errorf("Close() error = %v, want 'already closed'", err)
       }
   }

   func TestNbuClientCloseWaitsForActiveRequests(t *testing.T) {
       // Create a slow server
       requestStarted := make(chan struct{})
       requestComplete := make(chan struct{})

       server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
           close(requestStarted)
           <-requestComplete // Wait for test to signal completion
           w.Header().Set("Content-Type", "application/json")
           w.WriteHeader(http.StatusOK)
           json.NewEncoder(w).Encode(mockAPIResponse{})
       }))
       defer server.Close()

       cfg := createBasicTestConfig("13.0", "test-key")
       client := NewNbuClient(cfg)

       // Start a request in background
       var fetchErr error
       fetchDone := make(chan struct{})
       go func() {
           var result mockAPIResponse
           fetchErr = client.FetchData(context.Background(), server.URL, &result)
           close(fetchDone)
       }()

       // Wait for request to start
       <-requestStarted

       // Close should block waiting for request
       closeDone := make(chan struct{})
       go func() {
           client.Close()
           close(closeDone)
       }()

       // Give Close a moment to start waiting
       time.Sleep(50 * time.Millisecond)

       // Verify Close is still blocking
       select {
       case <-closeDone:
           t.Error("Close() returned before request completed")
       default:
           // Expected - Close is waiting
       }

       // Allow request to complete
       close(requestComplete)

       // Now Close should complete
       select {
       case <-closeDone:
           // Expected
       case <-time.After(5 * time.Second):
           t.Error("Close() did not complete after request finished")
       }

       // Verify request completed successfully
       <-fetchDone
       if fetchErr != nil {
           t.Errorf("FetchData() error: %v", fetchErr)
       }
   }

   func TestNbuClientFetchDataRejectsAfterClose(t *testing.T) {
       cfg := createBasicTestConfig("13.0", "test-key")
       client := NewNbuClient(cfg)

       // Close the client
       client.Close()

       // Attempt to fetch - should fail
       var result mockAPIResponse
       err := client.FetchData(context.Background(), "http://example.com", &result)
       if err == nil {
           t.Error("FetchData() after Close() expected error, got nil")
       }
       if !strings.Contains(err.Error(), "closed") {
           t.Errorf("FetchData() error = %v, want error containing 'closed'", err)
       }
   }
   ```

3. Add test for timeout during close:
   ```go
   func TestNbuClientCloseTimeout(t *testing.T) {
       // Create a server that never responds
       server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
           // Block indefinitely
           select {}
       }))
       defer server.Close()

       cfg := createBasicTestConfig("13.0", "test-key")
       client := NewNbuClient(cfg)

       // Start a request that will hang
       go func() {
           var result mockAPIResponse
           client.FetchData(context.Background(), server.URL, &result)
       }()

       // Give request time to start
       time.Sleep(50 * time.Millisecond)

       // Close with short timeout should return quickly
       ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)
       defer cancel()

       start := time.Now()
       err := client.CloseWithContext(ctx)
       elapsed := time.Since(start)

       // Should complete around timeout time, not hang
       if elapsed > 500*time.Millisecond {
           t.Errorf("CloseWithContext took %v, expected ~100ms timeout", elapsed)
       }

       if err == nil {
           t.Log("Close completed without error (request may have been cancelled)")
       }
   }
   ```
  </action>
  <verify>
Run `go test ./internal/exporter/... -run TestNbuClientClose -v` to verify close tests pass.
Run `go test ./internal/exporter/... -race` to verify no race conditions.
Run `go test ./... -v` to verify no regressions.
  </verify>
  <done>
Tests verify: (1) Close() is idempotent, (2) Close() waits for active requests, (3) FetchData rejects requests after Close(), (4) CloseWithContext respects timeout. Race detector passes.
  </done>
</task>

</tasks>

<verification>
1. Run full test suite: `go test ./... -v`
2. Run race detector: `go test ./... -race`
3. Build binary: `make cli`
4. Manual test: Start exporter, make scrape request, stop exporter (Ctrl+C), verify clean shutdown logs
</verification>

<success_criteria>
- NbuClient tracks active requests with atomic counter
- Close() waits for active requests to complete (up to 30s timeout)
- CloseWithContext() allows custom timeout control
- FetchData() rejects new requests after Close() is called
- Close() is idempotent (returns error on second call)
- All tests pass including race detection
- No connection leaks on repeated start/stop cycles
</success_criteria>

<output>
After completion, create `.planning/phases/01-critical-fixes-stability/01-03-SUMMARY.md`
</output>
