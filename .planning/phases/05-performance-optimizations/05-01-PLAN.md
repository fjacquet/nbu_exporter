---
phase: 05-performance-optimizations
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/exporter/netbackup.go
  - internal/exporter/netbackup_test.go
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Job fetching uses page size of 100 items per API call"
    - "All jobs in a page response are processed (not just first)"
    - "Pagination continues correctly with batch offset from API"
    - "For 1000 jobs, only ~10 API calls are made instead of 1000"
  artifacts:
    - path: "internal/exporter/netbackup.go"
      provides: "Batched job pagination with page limit 100"
      contains: "QueryParamLimit.*jobPageLimit"
    - path: "internal/exporter/netbackup_test.go"
      provides: "Tests for batch processing"
      contains: "TestFetchJobDetails.*batch"
  key_links:
    - from: "FetchJobDetails"
      to: "jobs.Data loop"
      via: "range iteration over all jobs"
      pattern: "for.*range jobs\\.Data"
---

<objective>
Increase job pagination page size from 1 to 100 items per API call

Purpose: The current implementation fetches ONE job per API call (`page[limit]=1`), causing 1000 API calls for 1000 jobs. NetBackup API supports up to 100 items per page. This change provides a 100x reduction in API calls, dramatically reducing scrape time.

Output: Modified FetchJobDetails that requests and processes 100 jobs per page instead of 1
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-performance-optimizations/05-RESEARCH.md
@internal/exporter/netbackup.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update FetchJobDetails for Batch Processing</name>
  <files>internal/exporter/netbackup.go</files>
  <action>
  Modify FetchJobDetails to fetch and process 100 jobs per API call:

  1. Add constant for job page limit (separate from storage):
     ```go
     const (
         jobPageLimit     = "100"  // Maximum allowed by NetBackup API for jobs
         // ... existing constants
     )
     ```

  2. Change QueryParamLimit in FetchJobDetails from "1" to jobPageLimit:
     ```go
     queryParams := map[string]string{
         QueryParamLimit:  jobPageLimit,  // Was "1", now "100"
         QueryParamOffset: strconv.Itoa(offset),
         // ... rest unchanged
     }
     ```

  3. Process ALL jobs in the response (replace single-job processing):
     ```go
     // Replace: job := jobs.Data[0] and single processing
     // With: Loop over all jobs
     for _, job := range jobs.Data {
         jobKey := JobMetricKey{
             Action:     job.Attributes.JobType,
             PolicyType: job.Attributes.PolicyType,
             Status:     strconv.Itoa(job.Attributes.Status),
         }

         statusKey := JobStatusKey{
             Action: job.Attributes.JobType,
             Status: strconv.Itoa(job.Attributes.Status),
         }

         jobsCount[jobKey]++
         jobsStatusCount[statusKey]++
         jobsSize[jobKey] += float64(job.Attributes.KilobytesTransferred * bytesPerKilobyte)
     }
     ```

  4. Update span attributes to reflect batch size:
     ```go
     attrs := []attribute.KeyValue{
         attribute.Int(telemetry.AttrNetBackupPageOffset, offset),
         attribute.Int(telemetry.AttrNetBackupJobsInPage, len(jobs.Data)),  // Actual count
     }
     ```
     Note: Remove AttrNetBackupPageNumber since it's meaningless with batches

  5. Keep existing pagination termination logic (API's Meta.Pagination.Next handles it)

  Key constraints:
  - Keep function signature unchanged for backward compatibility
  - Maintain graceful handling of empty pages (len(jobs.Data) == 0)
  - Use API's pagination metadata (Meta.Pagination.Next) for offset, don't compute manually
  </action>
  <verify>
  Run existing tests to ensure no regressions:
  ```bash
  go test ./internal/exporter/... -run "Job" -v
  ```
  All job-related tests should pass.
  </verify>
  <done>FetchJobDetails requests 100 jobs per page and processes all jobs in the response via range loop</done>
</task>

<task type="auto">
  <name>Task 2: Add Batch Processing Tests</name>
  <files>internal/exporter/netbackup_test.go</files>
  <action>
  Add tests to verify batch processing behavior:

  1. TestFetchJobDetails_BatchProcessing - Verifies multiple jobs are processed:
     ```go
     func TestFetchJobDetails_BatchProcessing(t *testing.T) {
         // Setup mock server returning 3 jobs in single response
         // Call FetchJobDetails once
         // Assert all 3 jobs were counted in maps
         // Assert only 1 API call was made
     }
     ```

  2. TestFetchJobDetails_BatchPagination - Verifies pagination with batches:
     ```go
     func TestFetchJobDetails_BatchPagination(t *testing.T) {
         // Setup mock server:
         //   - First call: returns 100 jobs, next offset = 100
         //   - Second call: returns 50 jobs, no next offset
         // Use FetchAllJobs to verify pagination
         // Assert total of 150 jobs counted
         // Assert exactly 2 API calls
     }
     ```

  3. TestFetchJobDetails_EmptyBatch - Verifies empty response handling:
     ```go
     func TestFetchJobDetails_EmptyBatch(t *testing.T) {
         // Setup mock returning empty jobs.Data
         // Verify returns -1 (end of pagination)
         // Verify no panic with empty slice
     }
     ```

  Use existing mock server patterns from the test file. Reference how TestFetchStorage or similar tests are structured.
  </action>
  <verify>
  Run new tests:
  ```bash
  go test ./internal/exporter/... -run "Batch" -v
  ```
  All batch tests should pass.
  </verify>
  <done>Tests verify that FetchJobDetails processes all jobs in a batch response and handles pagination correctly</done>
</task>

<task type="auto">
  <name>Task 3: Update Documentation and Run Full Test Suite</name>
  <files>internal/exporter/netbackup.go</files>
  <action>
  1. Update FetchJobDetails function comment to reflect batch behavior:
     ```go
     // FetchJobDetails retrieves a page of job records (up to 100) at the specified
     // pagination offset and updates the provided metrics maps with job statistics.
     // This function processes all jobs in the API response and uses the API's
     // pagination metadata to determine the next offset.
     //
     // Parameters:
     //   - offset: Pagination offset (starts at 0, use Meta.Pagination.Next for subsequent calls)
     // ...
     ```

  2. Update FetchAllJobs comment to mention batch efficiency:
     ```go
     // FetchAllJobs aggregates job statistics by iterating over paginated job data.
     // Uses batch pagination (100 jobs per request) to minimize API calls.
     // ...
     ```

  3. Run full test suite with race detector:
     ```bash
     go test -race ./...
     ```

  4. Verify build succeeds:
     ```bash
     go build ./...
     ```
  </action>
  <verify>
  ```bash
  go test -race ./... && go build ./...
  ```
  All tests pass, no race conditions, build succeeds.
  </verify>
  <done>Documentation updated, all tests pass with race detector, build succeeds</done>
</task>

</tasks>

<verification>
- [ ] QueryParamLimit uses "100" for job fetching (grep confirms)
- [ ] FetchJobDetails loops over jobs.Data (not just Data[0])
- [ ] Existing tests pass (no regressions)
- [ ] New batch tests pass
- [ ] Full test suite passes with race detector
- [ ] Build succeeds
</verification>

<success_criteria>
1. Job API calls reduced: For N jobs, approximately ceil(N/100) API calls instead of N
2. All jobs processed: Batch response fully processed, not just first item
3. Pagination works: Uses API's Meta.Pagination.Next correctly
4. Backward compatible: No changes to function signatures or return values
5. Tests pass: All existing and new tests pass with race detector
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-optimizations/05-01-SUMMARY.md`
</output>
