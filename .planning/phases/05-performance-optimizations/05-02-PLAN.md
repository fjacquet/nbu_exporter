---
phase: 05-performance-optimizations
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - internal/exporter/prometheus.go
  - go.mod
  - go.sum
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Storage and job metrics are fetched in parallel"
    - "Total scrape time is max(storage_time, jobs_time) not sum"
    - "Storage failure does not cancel job fetching (graceful degradation)"
    - "Job failure does not cancel storage fetching (graceful degradation)"
  artifacts:
    - path: "internal/exporter/prometheus.go"
      provides: "Parallel metric collection with errgroup"
      contains: "errgroup.WithContext"
    - path: "go.mod"
      provides: "errgroup dependency"
      contains: "golang.org/x/sync"
  key_links:
    - from: "collectAllMetrics"
      to: "collectStorageMetrics"
      via: "g.Go goroutine"
      pattern: "g\\.Go.*collectStorageMetrics"
    - from: "collectAllMetrics"
      to: "collectJobMetrics"
      via: "g.Go goroutine"
      pattern: "g\\.Go.*collectJobMetrics"
---

<objective>
Parallelize storage and job metric collection using errgroup

Purpose: Currently, storage metrics are fetched first, then job metrics after storage completes. By fetching both in parallel, total scrape time becomes max(storage_time, jobs_time) instead of their sum, providing 30-50% faster scrapes.

Output: Modified collectAllMetrics that runs storage and job collection concurrently while maintaining graceful degradation on partial failures
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-performance-optimizations/05-RESEARCH.md
@internal/exporter/prometheus.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add errgroup Dependency</name>
  <files>go.mod, go.sum</files>
  <action>
  Add the errgroup package to the project:

  ```bash
  go get golang.org/x/sync/errgroup
  ```

  This adds `golang.org/x/sync` to go.mod. The errgroup package provides:
  - `errgroup.WithContext()` - Creates group that cancels context on first error
  - `g.Go()` - Spawns a goroutine managed by the group
  - `g.Wait()` - Waits for all goroutines to complete

  Note: We'll use a specific pattern that does NOT cancel on error to maintain graceful degradation.
  </action>
  <verify>
  ```bash
  grep "golang.org/x/sync" go.mod
  ```
  Should show the sync module in go.mod.
  </verify>
  <done>errgroup dependency added to go.mod</done>
</task>

<task type="auto">
  <name>Task 2: Implement Parallel Collection in collectAllMetrics</name>
  <files>internal/exporter/prometheus.go</files>
  <action>
  Modify collectAllMetrics to use errgroup for parallel execution:

  1. Add import:
     ```go
     import (
         // ... existing imports
         "golang.org/x/sync/errgroup"
     )
     ```

  2. Replace sequential collection with parallel using errgroup:
     ```go
     func (c *NbuCollector) collectAllMetrics(ctx context.Context, span trace.Span) (
         storageMetrics []StorageMetricValue,
         jobsSize, jobsCount []JobMetricValue,
         jobsStatusCount []JobStatusMetricValue,
         storageErr, jobsErr error,
     ) {
         // Create errgroup with context for coordinated cancellation
         g, gCtx := errgroup.WithContext(ctx)

         // Collect storage metrics in parallel
         g.Go(func() error {
             storageMetrics, storageErr = c.collectStorageMetrics(gCtx, span)
             // Return nil to continue even if storage fails
             // This maintains graceful degradation: job collection continues
             return nil
         })

         // Collect job metrics in parallel
         g.Go(func() error {
             jobsSize, jobsCount, jobsStatusCount, jobsErr = c.collectJobMetrics(gCtx, span)
             // Return nil to continue even if jobs fail
             // This maintains graceful degradation: storage collection continues
             return nil
         })

         // Wait for both goroutines to complete
         // Since we always return nil, g.Wait() only returns nil or context error
         _ = g.Wait()

         return
     }
     ```

  Key design decisions:
  - ALWAYS return nil from g.Go() - errors tracked in storageErr/jobsErr
  - This prevents errgroup from canceling the context on one failure
  - Maintains existing graceful degradation behavior
  - Uses gCtx (group context) for proper cancellation propagation
  - Closure captures result variables safely (each goroutine writes different vars)
  </action>
  <verify>
  Run tests to ensure parallel execution works:
  ```bash
  go test ./internal/exporter/... -run "Collect" -v
  ```
  All collector tests should pass.
  </verify>
  <done>collectAllMetrics runs storage and job collection in parallel using errgroup</done>
</task>

<task type="auto">
  <name>Task 3: Add Parallel Collection Tests and Run Full Suite</name>
  <files>internal/exporter/prometheus.go</files>
  <action>
  1. Verify existing concurrent tests still pass (Phase 4 added these):
     ```bash
     go test ./internal/exporter/... -run "Concurrent" -v -race
     ```

  2. The existing tests in concurrent_test.go (TestCollectorConcurrentCollect, etc.)
     already verify thread safety of the collector. With parallel collection,
     these tests become even more important.

  3. Run full test suite with race detector to catch any races from parallel execution:
     ```bash
     go test -race ./...
     ```

  4. Verify the parallel pattern works with build:
     ```bash
     go build ./...
     ```

  Note: We don't need to add new tests specifically for parallel collection because:
  - Existing collector tests exercise the full Collect() -> collectAllMetrics path
  - Race detector catches any concurrency issues
  - The change is internal implementation (same interface)
  </action>
  <verify>
  ```bash
  go test -race ./... && go build ./...
  ```
  All tests pass with race detector, build succeeds.
  </verify>
  <done>All tests pass with race detector confirming parallel execution is thread-safe</done>
</task>

</tasks>

<verification>
- [ ] errgroup import present in prometheus.go
- [ ] collectAllMetrics uses errgroup.WithContext
- [ ] Both g.Go() calls return nil (graceful degradation)
- [ ] gCtx passed to both collectors (proper context)
- [ ] Existing tests pass (no regressions)
- [ ] Race detector passes
- [ ] Build succeeds
</verification>

<success_criteria>
1. Parallel execution: Storage and job collection run concurrently
2. Graceful degradation preserved: One failure doesn't cancel the other
3. Same interface: No changes to function signatures or return values
4. Thread-safe: Race detector finds no issues
5. All tests pass: Existing and concurrent tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-performance-optimizations/05-02-SUMMARY.md`
</output>
