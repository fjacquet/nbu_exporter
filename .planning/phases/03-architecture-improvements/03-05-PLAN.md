---
phase: 03-architecture-improvements
plan: 05
type: execute
wave: 4
depends_on: ["03-02", "03-03"]
files_modified:
  - internal/exporter/prometheus.go
  - main.go
autonomous: true

must_haves:
  truths:
    - "Server.Shutdown() closes NbuClient connections"
    - "Shutdown order documented: stop HTTP -> flush telemetry -> close client"
    - "No connection leaks on repeated stop/start cycles"
  artifacts:
    - path: "main.go"
      provides: "Server with integrated client lifecycle"
      contains: "collector.Close"
    - path: "internal/exporter/prometheus.go"
      provides: "NbuCollector with Close method"
      exports: ["Close"]
  key_links:
    - from: "main.go Server.Shutdown"
      to: "internal/exporter/prometheus.go NbuCollector.Close"
      via: "explicit call"
      pattern: "collector\\.Close"
---

<objective>
Integrate NbuClient lifecycle with Server shutdown, ensuring connections are properly closed and documenting the shutdown sequence.

Purpose: Implements FRAG-02 - connection pool lifecycle explicitly managed with documented cleanup requirements.

Output: NbuCollector.Close() method and integration with Server.Shutdown().
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-architecture-improvements/03-RESEARCH.md
@.planning/phases/03-architecture-improvements/03-02-SUMMARY.md

@internal/exporter/prometheus.go
@internal/exporter/client.go
@main.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Close method to NbuCollector</name>
  <files>internal/exporter/prometheus.go</files>
  <action>
Add Close() method to NbuCollector that closes the internal NbuClient:

```go
// Close releases resources associated with the collector, including
// closing the internal HTTP client connections.
//
// Shutdown Order (documented):
//   1. Stop accepting new scrapes (HTTP server stopped first)
//   2. Wait for active Collect() calls to complete
//   3. Close NbuClient (drains API connections)
//   4. Shutdown OpenTelemetry (flush traces)
//
// This method should be called during graceful shutdown after the
// HTTP server has stopped accepting requests.
//
// Returns an error if client closure fails.
func (c *NbuCollector) Close() error {
    if c.client != nil {
        return c.client.Close()
    }
    return nil
}

// CloseWithContext releases resources with explicit timeout control.
// Use this when you need custom shutdown timeout behavior.
//
// Parameters:
//   - ctx: Context for shutdown timeout
//
// Returns an error if client closure fails or context is cancelled.
func (c *NbuCollector) CloseWithContext(ctx context.Context) error {
    if c.client != nil {
        return c.client.CloseWithContext(ctx)
    }
    return nil
}
```
  </action>
  <verify>
- `go build ./internal/exporter/...` compiles
- `grep -n "func (c \*NbuCollector) Close" internal/exporter/prometheus.go` shows method
  </verify>
  <done>
NbuCollector.Close() method delegates to internal NbuClient.Close().
  </done>
</task>

<task type="auto">
  <name>Task 2: Update Server to track collector and close on shutdown</name>
  <files>main.go</files>
  <action>
Update Server struct and methods to properly manage collector lifecycle:

1. Add collector field to Server struct:
```go
type Server struct {
    cfg              models.Config
    httpSrv          *http.Server
    registry         *prometheus.Registry
    telemetryManager *telemetry.Manager
    serverErrChan    chan error
    collector        *exporter.NbuCollector  // Track collector for cleanup
}
```

2. Update Start() to store the collector reference:
```go
func (s *Server) Start() error {
    // ... existing telemetry initialization ...

    // Create NetBackup collector with injected TracerProvider
    var collectorOpts []exporter.CollectorOption
    if tracerProvider != nil {
        collectorOpts = append(collectorOpts, exporter.WithCollectorTracerProvider(tracerProvider))
    }

    collector, err := exporter.NewNbuCollector(s.cfg, collectorOpts...)
    if err != nil {
        return fmt.Errorf("failed to create collector: %w", err)
    }

    // Store collector reference for shutdown
    s.collector = collector

    // Register collector with Prometheus
    if err := s.registry.Register(collector); err != nil {
        return fmt.Errorf("failed to register collector: %w", err)
    }

    // ... rest of Start() unchanged ...
}
```

3. Update Shutdown() with documented order and client cleanup:
```go
// Shutdown gracefully shuts down the server components in the correct order.
//
// Shutdown Order:
//   1. Stop HTTP server (no new scrapes accepted)
//   2. Shutdown OpenTelemetry (flush pending spans)
//   3. Close collector (drains API connections)
//
// Note: Telemetry is shutdown BEFORE client to ensure traces from
// in-flight requests are flushed before connections close.
//
// Returns an error if shutdown fails or times out.
func (s *Server) Shutdown() error {
    var errs []error

    // Step 1: Shutdown HTTP server first (stops accepting new scrapes)
    if s.httpSrv != nil {
        ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
        defer cancel()

        log.Info("Shutting down HTTP server...")
        if err := s.httpSrv.Shutdown(ctx); err != nil {
            errs = append(errs, fmt.Errorf("HTTP server shutdown: %w", err))
        }
    }

    // Step 2: Shutdown OpenTelemetry (flush pending spans)
    if s.telemetryManager != nil {
        ctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
        defer cancel()

        log.Info("Shutting down telemetry...")
        if err := s.telemetryManager.Shutdown(ctx); err != nil {
            log.Warnf("Telemetry shutdown warning: %v", err)
            // Don't add to errs - telemetry shutdown warnings are non-fatal
        }
    }

    // Step 3: Close collector (drains API connections)
    if s.collector != nil {
        log.Info("Closing collector connections...")
        if err := s.collector.Close(); err != nil {
            errs = append(errs, fmt.Errorf("collector close: %w", err))
        }
    }

    // Close error channel to signal no more errors will be sent
    close(s.serverErrChan)

    if len(errs) > 0 {
        log.Errorf("Shutdown completed with %d errors", len(errs))
        // Return first error for simplicity
        return errs[0]
    }

    log.Info("Server stopped gracefully")
    return nil
}
```
  </action>
  <verify>
- `go build ./...` compiles
- `grep -n "s.collector.Close" main.go` shows cleanup call
- `grep -n "Shutdown Order" main.go` shows documentation
  </verify>
  <done>
Server.Shutdown() closes collector in documented order.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add integration test for shutdown sequence</name>
  <files>main.go</files>
  <action>
While full integration tests require a running NBU server, we can add a unit test for the shutdown sequence logic.

Since main.go tests are typically integration tests, document the expected behavior instead:

Add detailed documentation comment at package level or in main.go:

```go
// Shutdown Sequence Documentation
//
// The NBU Exporter follows a specific shutdown sequence to ensure clean resource cleanup:
//
// 1. HTTP Server Shutdown (10s timeout)
//    - Stops accepting new requests
//    - Waits for active requests to complete
//    - Any in-flight /metrics scrapes will complete
//
// 2. OpenTelemetry Shutdown (10s timeout)
//    - Flushes pending trace spans to collector
//    - Closes gRPC connections to OTLP endpoint
//    - Ensures traces from final scrapes are exported
//
// 3. Collector Close (30s default, configurable)
//    - Waits for active API requests to NbuClient
//    - Closes idle HTTP connections
//    - Releases connection pool resources
//
// This order ensures:
// - No new requests accepted after shutdown starts
// - All traces exported before connections close
// - Clean resource release with bounded timeouts
//
// To verify no connection leaks, monitor:
// - Active goroutines should return to baseline after shutdown
// - No "use of closed connection" errors in logs
// - Connection count metrics (if enabled) should go to zero
```

Add a test that verifies the Server struct has the collector field:
```go
// In main_test.go (if exists) or document as manual verification step
func TestServerHasCollectorField(t *testing.T) {
    cfg := models.Config{} // Minimal config
    server := NewServer(cfg)

    // Collector should be nil before Start
    if server.collector != nil {
        t.Error("collector should be nil before Start")
    }
}
```
  </action>
  <verify>
- `go build ./...` compiles
- Documentation exists in main.go explaining shutdown order
  </verify>
  <done>
Shutdown sequence documented, collector lifecycle integrated.
  </done>
</task>

<task type="auto">
  <name>Task 4: Test and commit</name>
  <files>internal/exporter/prometheus.go, main.go</files>
  <action>
1. Run full test suite:
```bash
go test ./... -race
```

2. Verify build:
```bash
go build ./...
```

3. Manual verification (if possible):
   - Start exporter with config
   - Send SIGTERM
   - Verify clean shutdown in logs

4. Commit:
```bash
git add internal/exporter/prometheus.go main.go
git commit -m "feat(03-05): integrate connection lifecycle with server shutdown

Implements FRAG-02: connection pool lifecycle explicitly managed.

- Add NbuCollector.Close() and CloseWithContext() methods
- Server stores collector reference for cleanup
- Shutdown order documented: HTTP -> Telemetry -> Client
- Ensures traces flushed before connections close
- No connection leaks on shutdown

Shutdown sequence:
1. Stop HTTP server (no new scrapes)
2. Flush OpenTelemetry spans
3. Close collector API connections

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```
  </action>
  <verify>
- `go test ./... -race` passes
- `git log -1 --oneline` shows commit
  </verify>
  <done>
Connection lifecycle fully integrated with server shutdown.
  </done>
</task>

</tasks>

<verification>
1. NbuCollector.Close() exists and delegates to NbuClient.Close()
2. Server.collector field stores reference for cleanup
3. Server.Shutdown() calls s.collector.Close()
4. Shutdown order is: HTTP server -> Telemetry -> Collector
5. Documentation explains shutdown sequence
6. All tests pass
</verification>

<success_criteria>
- NbuCollector has Close() method
- Server.Shutdown() closes collector connections
- Shutdown order documented in code
- Traces flushed before connections close
- No connection leaks on shutdown
- All tests pass with race detector
</success_criteria>

<output>
After completion, create `.planning/phases/03-architecture-improvements/03-05-SUMMARY.md`
</output>
